{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, seed_url):\n",
    "        self.queue = Queue()\n",
    "        self.visited = set()\n",
    "        self.seed_url = seed_url\n",
    "        self.queue.put(seed_url)\n",
    "\n",
    "    def crawl(self,data):\n",
    "        chrome_options = Options()\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        while not self.queue.empty():\n",
    "            url = self.queue.get()\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "\n",
    "            self.visited.add(url)\n",
    "            print(f\"Visiting {url}\")\n",
    "\n",
    "            # Selenium을 사용하여 페이지를 렌더링\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                # 페이지가 완전히 로드될 때까지 기다림\n",
    "                driver.implicitly_wait(10)\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                links = soup.find_all(\"a\", href=True)\n",
    "                # CSV 파일에 데이터를 추가하는 함수\n",
    "                title = soup.title.string if soup.title else 'No Title'\n",
    "                html = driver.page_source\n",
    "                text = soup.get_text()\n",
    "                data.append([title,url,text])\n",
    "                for link in links:\n",
    "                    href = link[\"href\"]\n",
    "                    if 'onclick' in link.attrs:\n",
    "                        driver.execute_script(link.attrs['onclick'])\n",
    "                        driver.implicitly_wait(2)\n",
    "                        html = driver.page_source\n",
    "                        title = soup.title.string if soup.title else 'No Title'\n",
    "                        text = soup.get_text()\n",
    "                        data.append([title,url,text])\n",
    "                    # 상대 경로를 절대 경로로 변환\n",
    "                    absolute_url = urljoin(url, href)\n",
    "                    if absolute_url not in self.visited and absolute_url.startswith(seed_url):\n",
    "                        self.queue.put(absolute_url)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled page: https://recruit.snowcorp.com/rcrt/list.do start_index 0\n",
      "total links: 96\n",
      "No alert in new window\n",
      "\"https://recruit.snowcorp.com/rcrt/list.do_link_21\"[Link: 21]\n",
      "96\n",
      "Executing onclick JavaScript for link 21 at https://recruit.snowcorp.com/rcrt/list.do: show('30003063')\n",
      "End onclick JavaScript for link 21 at https://recruit.snowcorp.com/rcrt/list.do: show('30003063')\n",
      "add new_url https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr=\n",
      "Saved 2 pages to crawled_data.json\n",
      "save results\n",
      "Crawled page: https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr= start_index 0\n",
      "total links: 34\n",
      "No alert in new window\n",
      "\"https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr=_link_27\"[Link: 27]\n",
      "34\n",
      "Executing onclick JavaScript for link 27 at https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr=: apply();\n",
      "End onclick JavaScript for link 27 at https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr=: apply();\n",
      "Error processing link 27 at https://recruit.snowcorp.com/rcrt/view.do?annoId=30003063&sw=&subJobCdArr=&sysCompanyCdArr=&empTypeCdArr=&entTypeCdArr=&workAreaCdArr=: HTTPConnectionPool(host='localhost', port=62050): Max retries exceeded with url: /session/19cda06f727d1b0acfb973b7015ec484/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x11f956a50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 21 at https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62050): Max retries exceeded with url: /session/19cda06f727d1b0acfb973b7015ec484/window/handles (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x11f954050>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error crawling https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62050): Max retries exceeded with url: /session/19cda06f727d1b0acfb973b7015ec484/window/handles (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x11f957530>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Graph saved to crawler_graph.mmd\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "seed_url =\"https://recruit.navercorp.com/\"\n",
    "crawler = Crawler(seed_url)\n",
    "crawler.crawl(data)\n",
    "df = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"Text\"])\n",
    "df.to_csv('navercorp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"navercorp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def _explore_page(self, current_url, start_index=0):\n",
    "    print(f\"Crawled page: {current_url} start_index {start_index}\")\n",
    "    if current_url not in self.link_cache:\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "            links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            self.link_cache[current_url] = [\n",
    "                {\n",
    "                    \"href\": link.get_attribute(\"href\"),\n",
    "                    \"onclick\": link.get_attribute(\"onclick\"),\n",
    "                }\n",
    "                for link in links\n",
    "            ]\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout waiting for links on {current_url}\")\n",
    "            return []\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "        )\n",
    "        links = self.driver.find_elements(By_TAG_NAME, \"a\")\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout reloading links on {current_url}\")\n",
    "        return []\n",
    "\n",
    "    new_urls = []\n",
    "    original_window = self.driver.current_window_handle\n",
    "    print(f\"Original window handle: {original_window}\")\n",
    "    print(\"total links:\", len(links))\n",
    "\n",
    "    def execute_js_with_timeout(driver, script, element, timeout=5):\n",
    "        \"\"\"JavaScript 실행에 타임아웃을 적용하여 멈춤 방지\"\"\"\n",
    "        result = [None]\n",
    "        exception = [None]\n",
    "\n",
    "        def target():\n",
    "            try:\n",
    "                result[0] = driver.execute_script(script, element)\n",
    "            except Exception as e:\n",
    "                exception[0] = e\n",
    "\n",
    "        thread = threading.Thread(target=target)\n",
    "        thread.start()\n",
    "        thread.join(timeout)\n",
    "        if thread.is_alive():\n",
    "            print(f\"JavaScript execution timed out after {timeout} seconds\")\n",
    "            return None, TimeoutError(\"Script execution timed out\")\n",
    "        return result[0], exception[0]\n",
    "\n",
    "    for i in range(start_index, len(links)):\n",
    "        try:\n",
    "            link_data = links[i]\n",
    "            href = link_data.get_attribute(\"href\")\n",
    "            onclick = link_data.get_attribute(\"onclick\")\n",
    "            link_node = f'\"{current_url}_link_{i}\"[Link: {i}]'\n",
    "\n",
    "            if not onclick:\n",
    "                continue\n",
    "\n",
    "            if len(self.driver.window_handles) >= self.max_tabs:\n",
    "                print(f\"Max tabs reached: {self.max_tabs}\")\n",
    "                continue\n",
    "\n",
    "            # 새 창 열기\n",
    "            print(f\"Opening new window for link {i}\")\n",
    "            self.driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "            new_window = self.driver.window_handles[-1]\n",
    "            print(f\"New window handle: {new_window}\")\n",
    "            self.driver.switch_to.window(new_window)\n",
    "            self.driver.get(current_url)\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "            link_data = self.driver.find_elements(By_TAG_NAME, \"a\")[i]\n",
    "            print(f\"Preparing to execute onclick for link {i}: {onclick}\")\n",
    "\n",
    "            # JavaScript 실행에 타임아웃 적용\n",
    "            result, error = execute_js_with_timeout(self.driver, onclick, link_data, timeout=5)\n",
    "            if error:\n",
    "                print(f\"Error executing onclick for link {i}: {str(error)}\")\n",
    "                if isinstance(error, TimeoutError):\n",
    "                    # 타임아웃 시 창 정리 후 복귀\n",
    "                    self.driver.close()\n",
    "                    self.driver.switch_to.window(original_window)\n",
    "                    continue\n",
    "\n",
    "            # 실행 후 상태 확인\n",
    "            print(f\"onclick executed for link {i}\")\n",
    "            time.sleep(1)  # 페이지 로드 대기\n",
    "            new_url = self.driver.current_url\n",
    "            print(f\"New URL after onclick: {new_url}\")\n",
    "\n",
    "            # Alert 처리\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 3).until(EC.alert_is_present())\n",
    "                print(f\"Alert accepted on link {i}\")\n",
    "            except TimeoutException:\n",
    "                print(f\"No alert present on link {i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error handling alert on link {i}: {str(e)}\")\n",
    "\n",
    "            # 팝업창 처리\n",
    "            current_handles = self.driver.window_handles\n",
    "            print(f\"Current window handles: {current_handles}\")\n",
    "            if len(current_handles) > 2:\n",
    "                for handle in current_handles:\n",
    "                    if handle != original_window and handle != new_window:\n",
    "                        print(f\"Switching to popup: {handle}\")\n",
    "                        self.driver.switch_to.window(handle)\n",
    "                        print(f\"Popup URL: {self.driver.current_url}\")\n",
    "                        self.driver.close()\n",
    "                        print(f\"Closed popup: {handle}\")\n",
    "\n",
    "            # 새 창으로 다시 전환\n",
    "            self.driver.switch_to.window(new_window)\n",
    "            new_url = self.driver.current_url\n",
    "\n",
    "            # 새 URL 처리\n",
    "            if (\n",
    "                new_url != current_url\n",
    "                and new_url not in self.visited_urls\n",
    "                and self._is_within_domain(new_url)\n",
    "            ):\n",
    "                page_data = self._collect_page_data(new_url)\n",
    "                self.crawled_data.append(page_data)\n",
    "                self._save_results()\n",
    "                print(f\"Saved new page: {new_url}\")\n",
    "                self.visited_urls.add(new_url)\n",
    "                new_urls.append(new_url)\n",
    "                self.graph_nodes.add(f'\"{new_url}\"[Page: {new_url}]')\n",
    "                self.graph_edges.append(f'{link_node} --> |Onclick| \"{new_url}\"')\n",
    "\n",
    "                recursive_urls = self._explore_page(new_url, start_index=0)\n",
    "                new_urls.extend(\n",
    "                    [url for url in recursive_urls if url not in self.visited_urls]\n",
    "                )\n",
    "\n",
    "            # 새 창 닫고 복귀\n",
    "            print(f\"Closing new window: {new_window}\")\n",
    "            self.driver.close()\n",
    "            self.driver.switch_to.window(original_window)\n",
    "            WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link {i} at {current_url}: {str(e)}\")\n",
    "            try:\n",
    "                current_handles = self.driver.window_handles\n",
    "                print(f\"Handles during recovery: {current_handles}\")\n",
    "                for handle in current_handles:\n",
    "                    if handle != original_window:\n",
    "                        self.driver.switch_to.window(handle)\n",
    "                        self.driver.close()\n",
    "                self.driver.switch_to.window(original_window)\n",
    "                WebDriverWait(self.driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "                )\n",
    "            except Exception as recovery_error:\n",
    "                print(f\"Failed to recover: {str(recovery_error)}\")\n",
    "                return new_urls\n",
    "\n",
    "    return new_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdvancedWebCrawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crawler \u001b[38;5;241m=\u001b[39m \u001b[43mAdvancedWebCrawler\u001b[49m(base_domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://recruit.snowcorp.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m start_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://recruit.snowcorp.com/rcrt/list.do\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m crawler\u001b[38;5;241m.\u001b[39mcrawl(start_url, max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdvancedWebCrawler' is not defined"
     ]
    }
   ],
   "source": [
    "crawler = AdvancedWebCrawler(base_domain=\"https://recruit.snowcorp.com\")\n",
    "start_url = \"https://recruit.snowcorp.com/rcrt/list.do\"\n",
    "crawler.crawl(start_url, max_pages=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crwoller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
