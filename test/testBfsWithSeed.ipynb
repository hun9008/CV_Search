{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, seed_url):\n",
    "        self.queue = Queue()\n",
    "        self.visited = set()\n",
    "        self.seed_url = seed_url\n",
    "        self.queue.put(seed_url)\n",
    "\n",
    "    def crawl(self,data):\n",
    "        chrome_options = Options()\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        while not self.queue.empty():\n",
    "            url = self.queue.get()\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "\n",
    "            self.visited.add(url)\n",
    "            print(f\"Visiting {url}\")\n",
    "\n",
    "            # Selenium을 사용하여 페이지를 렌더링\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                # 페이지가 완전히 로드될 때까지 기다림\n",
    "                driver.implicitly_wait(10)\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                links = soup.find_all(\"a\", href=True)\n",
    "                # CSV 파일에 데이터를 추가하는 함수\n",
    "                title = soup.title.string if soup.title else 'No Title'\n",
    "                html = driver.page_source\n",
    "                text = soup.get_text()\n",
    "                data.append([title,url,text])\n",
    "                for link in links:\n",
    "                    href = link[\"href\"]\n",
    "                    if 'onclick' in link.attrs:\n",
    "                        driver.execute_script(link.attrs['onclick'])\n",
    "                        driver.implicitly_wait(2)\n",
    "                        html = driver.page_source\n",
    "                        title = soup.title.string if soup.title else 'No Title'\n",
    "                        text = soup.get_text()\n",
    "                        data.append([title,url,text])\n",
    "                    # 상대 경로를 절대 경로로 변환\n",
    "                    absolute_url = urljoin(url, href)\n",
    "                    if absolute_url not in self.visited and absolute_url.startswith(seed_url):\n",
    "                        self.queue.put(absolute_url)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "seed_url =\"https://recruit.navercorp.com/\"\n",
    "crawler = Crawler(seed_url)\n",
    "crawler.crawl(data)\n",
    "df = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"Text\"])\n",
    "df.to_csv('navercorp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"navercorp.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crwoller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
