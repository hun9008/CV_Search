{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "montior threads now...Started continuous thread monitoring\n",
      "\n",
      "Stopped continuous thread monitoring\n",
      "Saved 0 pages to crawled_data.json\n",
      "Graph saved to crawler_graph.mmd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread Thread-27 starting crawl: https://recruit.snowcorp.com/rcrt/list.do\n",
      "Crawled page: https://recruit.snowcorp.com/rcrt/list.do start_index 0\n",
      "Stale element at index 22 on https://recruit.snowcorp.com/rcrt/list.do, retrying...\n",
      "Error executing onclick at https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107ba2270>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 28 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107ba1c70>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 29 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107ba0980>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 30 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107ba0710>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 31 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107384e60>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 32 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107b09550>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 33 on https://recruit.snowcorp.com/rcrt/list.do: HTTPConnectionPool(host='localhost', port=62872): Max retries exceeded with url: /session/54e6c3edbb2f34fc4bf6568494109775/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x107b08b00>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error processing link 34 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 35 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 36 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 37 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 38 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 39 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 40 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 41 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 42 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 43 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 44 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 45 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 46 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 47 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 48 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 49 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 50 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 51 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 52 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 53 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 54 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 55 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 56 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 57 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 58 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 59 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 60 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 61 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 62 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 63 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 64 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 65 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 66 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 67 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 68 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 69 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 70 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 71 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 72 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 73 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 74 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 75 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 76 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 77 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 78 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 79 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 80 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 81 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 82 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 83 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 84 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 85 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 86 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 87 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 88 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 89 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 90 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 91 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 92 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 93 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 94 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Error processing link 95 on https://recruit.snowcorp.com/rcrt/list.do: list index out of range\n",
      "Thread Thread-27 finished\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchWindowException,\n",
    "    InvalidSessionIdException,\n",
    "    StaleElementReferenceException,\n",
    ")\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "class CrawlerThread(threading.Thread):\n",
    "    def __init__(self, url, result_queue, crawler_instance):\n",
    "        super().__init__()\n",
    "        self.url = url\n",
    "        self.result_queue = result_queue\n",
    "        self.crawler = crawler_instance\n",
    "        self.driver = None\n",
    "        self.is_stopped = threading.Event()\n",
    "\n",
    "    def setup_driver(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()), options=options\n",
    "        )\n",
    "        self.driver.implicitly_wait(5)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "            print(f\"Thread {self.name} starting crawl: {self.url}\")\n",
    "\n",
    "            self.driver.get(self.url)\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                self.driver.execute_script(\n",
    "                    \"document.querySelector('.fake-alert-class').remove();\"\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            page_data = self.crawler._collect_page_data(self.driver, self.url)\n",
    "            new_urls = self.crawler._explore_page(self.driver, self.url, start_index=0)\n",
    "\n",
    "            self.result_queue.put(\n",
    "                {\"thread_name\": self.name, \"page_data\": page_data, \"new_urls\": new_urls}\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Thread {self.name} encountered an error: {e}\")\n",
    "            self.result_queue.put({\"thread_name\": self.name, \"error\": str(e)})\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "            print(f\"Thread {self.name} finished\")\n",
    "\n",
    "    def stop(self):\n",
    "        self.is_stopped.set()\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "        print(f\"Thread {self.name} stopped\")\n",
    "\n",
    "\n",
    "class AdvancedWebCrawler:\n",
    "    def __init__(self, base_domain=\"https://recruit.snowcorp.com\"):\n",
    "        self.base_domain = urlparse(base_domain).netloc\n",
    "        self.visited_urls = set()\n",
    "        self.crawled_data = []\n",
    "        self.graph_nodes = set()\n",
    "        self.graph_edges = []\n",
    "        self.link_cache = {}\n",
    "        self.max_tabs = 3\n",
    "        self.result_queue = Queue()\n",
    "        self.timeout = 10\n",
    "        self.data_lock = Lock()\n",
    "        self.monitoring_active = threading.Event()  # 모니터링 상태 플래그\n",
    "        self.all_threads = []  # 모든 크롤링 스레드 저장\n",
    "\n",
    "    def _is_within_domain(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        return parsed_url.netloc == self.base_domain\n",
    "\n",
    "    def _collect_page_data(self, driver, url):\n",
    "        try:\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": driver.title,\n",
    "                \"text\": driver.find_element(By.TAG_NAME, \"body\").text,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting page data for {url}: {e}\")\n",
    "            return {\"url\": url, \"title\": \"Unknown\", \"text\": \"\"}\n",
    "\n",
    "    def monitor_threads(self, timeout=10):\n",
    "        \"\"\"지속적으로 스레드를 감시하는 백그라운드 작업\"\"\"\n",
    "        while self.monitoring_active.is_set():\n",
    "            print(\"montior threads now...\")\n",
    "            for thread in self.all_threads[:]:  # 리스트 복사본 사용\n",
    "                if thread.is_alive():\n",
    "                    try:\n",
    "                        thread.driver.title\n",
    "                    except:\n",
    "                        print(f\"Detected stalled driver in {thread.name}\")\n",
    "                        thread.stop()\n",
    "                        self.all_threads.remove(thread)  # 종료된 스레드 제거\n",
    "                elif thread in self.all_threads:\n",
    "                    self.all_threads.remove(thread)  # 종료된 스레드 제거\n",
    "            time.sleep(1)  # 감시 간격\n",
    "\n",
    "    def start_monitoring(self):\n",
    "        \"\"\"모니터링 스레드 시작\"\"\"\n",
    "        self.monitoring_active.set()\n",
    "        monitor_thread = threading.Thread(target=self.monitor_threads, args=(10,))\n",
    "        monitor_thread.daemon = True  # 메인 스레드 종료 시 함께 종료\n",
    "        monitor_thread.start()\n",
    "        print(\"Started continuous thread monitoring\")\n",
    "\n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"모니터링 스레드 중지\"\"\"\n",
    "        self.monitoring_active.clear()\n",
    "        print(\"Stopped continuous thread monitoring\")\n",
    "\n",
    "    def crawl(self, start_url, max_pages=100):\n",
    "        url_queue = [start_url]\n",
    "        pages_crawled = 0\n",
    "\n",
    "        # 모니터링 시작\n",
    "        self.start_monitoring()\n",
    "\n",
    "        try:\n",
    "            while url_queue and pages_crawled < max_pages:\n",
    "                current_url = url_queue.pop(0)\n",
    "\n",
    "                if current_url in self.visited_urls or not self._is_within_domain(\n",
    "                    current_url\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                thread = CrawlerThread(current_url, self.result_queue, self)\n",
    "                self.all_threads.append(thread)\n",
    "                thread.start()\n",
    "\n",
    "                # 결과 처리 (비동기적으로 큐에서 데이터 수집)\n",
    "                while not self.result_queue.empty():\n",
    "                    result = self.result_queue.get()\n",
    "                    if \"error\" in result:\n",
    "                        print(f\"Error in {result['thread_name']}: {result['error']}\")\n",
    "                        continue\n",
    "\n",
    "                    with self.data_lock:\n",
    "                        self.crawled_data.append(result[\"page_data\"])\n",
    "                        self.visited_urls.add(result[\"page_data\"][\"url\"])\n",
    "                        self.graph_nodes.add(\n",
    "                            f'\"{result[\"page_data\"][\"url\"]}\"[Page: {result[\"page_data\"][\"url\"]}]'\n",
    "                        )\n",
    "                    pages_crawled += 1\n",
    "\n",
    "                    new_urls = result[\"new_urls\"]\n",
    "                    url_queue.extend(\n",
    "                        [url for url in new_urls if url not in self.visited_urls]\n",
    "                    )\n",
    "\n",
    "                time.sleep(0.1)  # CPU 과부하 방지\n",
    "\n",
    "        finally:\n",
    "            # 크롤링 종료 시 모든 스레드 기다리고 모니터링 중지\n",
    "            for thread in self.all_threads:\n",
    "                thread.join()\n",
    "            self.stop_monitoring()\n",
    "            self._save_results()\n",
    "            self._save_graph()\n",
    "\n",
    "    def _explore_page(self, driver, current_url, start_index=0):\n",
    "        print(f\"Crawled page: {current_url} start_index {start_index}\")\n",
    "        if current_url not in self.link_cache:\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "                )\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                self.link_cache[current_url] = [\n",
    "                    {\n",
    "                        \"href\": link.get_attribute(\"href\"),\n",
    "                        \"onclick\": link.get_attribute(\"onclick\"),\n",
    "                    }\n",
    "                    for link in links\n",
    "                ]\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout waiting for links on {current_url}\")\n",
    "                return []\n",
    "\n",
    "        new_urls = []\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout reloading links on {current_url}\")\n",
    "            return []\n",
    "\n",
    "        for i in range(start_index, len(links)):\n",
    "            try:\n",
    "                link_data = links[i]\n",
    "                href = link_data.get_attribute(\"href\")\n",
    "                onclick = link_data.get_attribute(\"onclick\")\n",
    "                link_node = f'\"{current_url}_link_{i}\"[Link: {i}]'\n",
    "\n",
    "                if (\n",
    "                    href\n",
    "                    and self._is_within_domain(href)\n",
    "                    and href not in self.visited_urls\n",
    "                ):\n",
    "                    new_urls.append(href)\n",
    "                    self.graph_edges.append(f'\"{current_url}\" --> |Href| \"{href}\"')\n",
    "\n",
    "                if onclick:\n",
    "                    try:\n",
    "                        driver.execute_script(onclick, link_data)\n",
    "                        time.sleep(1)\n",
    "                        new_url = driver.current_url\n",
    "                        if new_url != current_url and self._is_within_domain(new_url):\n",
    "                            new_urls.append(new_url)\n",
    "                            self.graph_edges.append(\n",
    "                                f'{link_node} --> |Onclick| \"{new_url}\"'\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing onclick at {current_url}: {e}\")\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                print(f\"Stale element at index {i} on {current_url}, retrying...\")\n",
    "                try:\n",
    "                    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                    if i < len(links):\n",
    "                        link_data = links[i]\n",
    "                        href = link_data.get_attribute(\"href\")\n",
    "                        onclick = link_data.get_attribute(\"onclick\")\n",
    "                        link_node = f'\"{current_url}_link_{i}\"[Link: {i}]'\n",
    "\n",
    "                        if (\n",
    "                            href\n",
    "                            and self._is_within_domain(href)\n",
    "                            and href not in self.visited_urls\n",
    "                        ):\n",
    "                            new_urls.append(href)\n",
    "                            self.graph_edges.append(\n",
    "                                f'\"{current_url}\" --> |Href| \"{href}\"'\n",
    "                            )\n",
    "                    else:\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Retry failed for index {i} on {current_url}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {i} on {current_url}: {e}\")\n",
    "\n",
    "        return new_urls\n",
    "\n",
    "    def _save_results(self):\n",
    "        with open(\"crawled_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.crawled_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(self.crawled_data)} pages to crawled_data.json\")\n",
    "\n",
    "    def _save_graph(self):\n",
    "        with open(\"crawler_graph.mmd\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"graph TD\\n\")\n",
    "            for node in self.graph_nodes:\n",
    "                f.write(f\"    {node}\\n\")\n",
    "            for edge in self.graph_edges:\n",
    "                f.write(f\"    {edge}\\n\")\n",
    "        print(\"Graph saved to crawler_graph.mmd\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    crawler = AdvancedWebCrawler(base_domain=\"https://recruit.snowcorp.com\")\n",
    "    start_url = \"https://recruit.snowcorp.com/rcrt/list.do\"\n",
    "    crawler.crawl(start_url, max_pages=10)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crwoller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
